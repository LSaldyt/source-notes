# Three ideal observer models for rule learning in simple languages

### Summary

Rule learning in language is important for the simplest forms of human generalization.  
This paper reviews hierarchical bayesian models of rule learning.  
"These models capture a wide range of experimental results -- including several that have been used to argue for domain-specificity or limits on the kinds of generalizations learners can make -- suggesting that these ideal observers may be a useful baseline for future work in rule learning"  
This supports the psychological plausibility of hierarchical bayesian networks in the human mind.  
For more on this, see the "Optimal Predictions" paper by Tenenbaum's group.  
These results also revise the simple "rules vs statistics" debate in language aquisition, suggesting that, "On empirical grounds, there is support for both the availability of rule-like representations and the ability of learners to perform statistical inferences over these representations".   
Generalizing, this supports Tenenbaum et al's general idea that cognitive models need to combine explicit and implicit representations.

### Hypotheses

Modern language models (which historically take very symbolic approaches) should admit nuance and allow rules and statistics to interact in their models.  
In its extreme version, it is possible that language aquisition is perhaps more weighted towards statistics than towards explicit rules.  

IE: Statistics are required for guiding generalization from sparse data.  

### Methods 

Three models of language acquisition are created: a single rule learner, a single rule learner that admits noise, and a multiple rule learner that admits noise.  
Each of these is compared to several studies in infant language acquisition of the same rules.  

Rules learned are _incredibly_ simple: either ABA or ABB.

### Results

In general, these models perform similarly to infants.  
Model 3 (the most sophisticated) is able to complete every task that infants are able to complete.  
However, this work is admitted to only be baseline work, and doesn't provide discussion-ending results, especially because the rules are so simple.

### Discussion

As admitted, this is only a baseline paper. However, the rules learned are so simple that it may be too early to make sweeping claims.  
However, this model will likely be improved in the future, and tested on more complicated rules.  
It is very likely that future work done with this kind of model will be very productive and insightful.

### Takeaways:

What this paper does well is the comparisons to humans: It brings in results from nine separate custom human experiments to compare against multiple computer models.  
This is pretty similar to the copycat cross-comparison test-structure, if only the human data were better.  

A radical takeaway would be the opinions on symbolic v implicit representations.  
I know that FARG models use procedural representations of concepts, but perhaps future integration of implicit representations will be helpful for some kinds of models?   

### Citation:

```
@article{simple-rule-learning,
    author  = "Frank, Michael C.. Tenenbaum, Joshua B.",
    title   = "Three ideal observer models for rule learning in simple languages",
    journal = "Cognition",
    volume  = "",
    number  = "",
    pages   = "",
    year    = "2010",
    DOI     = "10.1016/j.cognition.2010.10.005"
    url     = "http://langcog.stanford.edu/papers/FT-cognition2011.pdf"
}
```
